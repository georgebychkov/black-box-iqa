diff --git a/dover/datasets/dover_datasets.py b/dover/datasets/dover_datasets.py
index 28e72ec..d421cdf 100644
--- a/dover/datasets/dover_datasets.py
+++ b/dover/datasets/dover_datasets.py
@@ -227,37 +227,23 @@ def get_single_view(
 
 
 def spatial_temporal_view_decomposition(
-    video_path, sample_types, samplers, is_train=False, augment=False,
+    batch, sample_types, samplers, is_train=False, augment=False,
 ):
     video = {}
-    if video_path.endswith(".yuv"):
-        print("This part will be deprecated due to large memory cost.")
-        ## This is only an adaptation to LIVE-Qualcomm
-        ovideo = skvideo.io.vread(
-            video_path, 1080, 1920, inputdict={"-pix_fmt": "yuvj420p"}
-        )
-        for stype in samplers:
-            frame_inds = samplers[stype](ovideo.shape[0], is_train)
-            imgs = [torch.from_numpy(ovideo[idx]) for idx in frame_inds]
-            video[stype] = torch.stack(imgs, 0).permute(3, 0, 1, 2)
-        del ovideo
-    else:
-        decord.bridge.set_bridge("torch")
-        vreader = VideoReader(video_path)
-        ### Avoid duplicated video decoding!!! Important!!!!
-        all_frame_inds = []
-        frame_inds = {}
-        for stype in samplers:
-            frame_inds[stype] = samplers[stype](len(vreader), is_train)
-            all_frame_inds.append(frame_inds[stype])
-
-        ### Each frame is only decoded one time!!!
-        all_frame_inds = np.concatenate(all_frame_inds, 0)
-        frame_dict = {idx: vreader[idx] for idx in np.unique(all_frame_inds)}
-
-        for stype in samplers:
-            imgs = [frame_dict[idx] for idx in frame_inds[stype]]
-            video[stype] = torch.stack(imgs, 0).permute(3, 0, 1, 2)
+    ### Avoid duplicated video decoding!!! Important!!!!
+    all_frame_inds = []
+    frame_inds = {}
+    for stype in samplers:
+        frame_inds[stype] = samplers[stype](len(batch), is_train)
+        all_frame_inds.append(frame_inds[stype])
+
+    ### Each frame is only decoded one time!!!
+    all_frame_inds = np.concatenate(all_frame_inds, 0)
+    frame_dict = {idx: batch[idx] for idx in np.unique(all_frame_inds)}
+
+    for stype in samplers:
+        imgs = [frame_dict[idx] for idx in frame_inds[stype]]
+        video[stype] = torch.stack(imgs, 0).permute(1, 0, 2, 3)
 
     sampled_video = {}
     for stype, sopt in sample_types.items():
